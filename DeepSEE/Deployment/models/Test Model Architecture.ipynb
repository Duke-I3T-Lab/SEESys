{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.36.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS regression\n",
    "## - Time series only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PatchTSMixerConfig, PatchTSMixerForRegression\n",
    "#from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_TS = PatchTSMixerConfig(context_length = 30, \n",
    "                            prediction_length = 1,\n",
    "                            num_input_channels = 13, \n",
    "                            num_targets = 1,\n",
    "                            patch_len = 10,\n",
    "                            patch_stride = 5,\n",
    "                            use_positional_encoding = True,\n",
    "                            output_range=[0,1]\n",
    "                           )\n",
    "model_TS = PatchTSMixerForRegression(config_TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchTSMixerForRegression(\n",
       "  (model): PatchTSMixerModel(\n",
       "    (encoder): PatchTSMixerEncoder(\n",
       "      (patcher): Linear(in_features=10, out_features=8, bias=True)\n",
       "      (positional_encoder): PatchTSMixerPositionalEncoding()\n",
       "      (mlp_mixer_encoder): PatchTSMixerBlock(\n",
       "        (mixers): ModuleList(\n",
       "          (0): PatchTSMixerLayer(\n",
       "            (patch_mixer): PatchMixerBlock(\n",
       "              (norm): PatchTSMixerNormLayer(\n",
       "                (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (mlp): PatchTSMixerMLP(\n",
       "                (fc1): Linear(in_features=5, out_features=10, bias=True)\n",
       "                (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                (fc2): Linear(in_features=10, out_features=5, bias=True)\n",
       "                (dropout2): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gating_block): PatchTSMixerGatedAttention(\n",
       "                (attn_layer): Linear(in_features=5, out_features=5, bias=True)\n",
       "                (attn_softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (feature_mixer): FeatureMixerBlock(\n",
       "              (norm): PatchTSMixerNormLayer(\n",
       "                (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (mlp): PatchTSMixerMLP(\n",
       "                (fc1): Linear(in_features=8, out_features=16, bias=True)\n",
       "                (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
       "                (dropout2): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gating_block): PatchTSMixerGatedAttention(\n",
       "                (attn_layer): Linear(in_features=8, out_features=8, bias=True)\n",
       "                (attn_softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): PatchTSMixerLayer(\n",
       "            (patch_mixer): PatchMixerBlock(\n",
       "              (norm): PatchTSMixerNormLayer(\n",
       "                (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (mlp): PatchTSMixerMLP(\n",
       "                (fc1): Linear(in_features=5, out_features=10, bias=True)\n",
       "                (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                (fc2): Linear(in_features=10, out_features=5, bias=True)\n",
       "                (dropout2): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gating_block): PatchTSMixerGatedAttention(\n",
       "                (attn_layer): Linear(in_features=5, out_features=5, bias=True)\n",
       "                (attn_softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (feature_mixer): FeatureMixerBlock(\n",
       "              (norm): PatchTSMixerNormLayer(\n",
       "                (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (mlp): PatchTSMixerMLP(\n",
       "                (fc1): Linear(in_features=8, out_features=16, bias=True)\n",
       "                (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
       "                (dropout2): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gating_block): PatchTSMixerGatedAttention(\n",
       "                (attn_layer): Linear(in_features=8, out_features=8, bias=True)\n",
       "                (attn_softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): PatchTSMixerLayer(\n",
       "            (patch_mixer): PatchMixerBlock(\n",
       "              (norm): PatchTSMixerNormLayer(\n",
       "                (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (mlp): PatchTSMixerMLP(\n",
       "                (fc1): Linear(in_features=5, out_features=10, bias=True)\n",
       "                (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                (fc2): Linear(in_features=10, out_features=5, bias=True)\n",
       "                (dropout2): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gating_block): PatchTSMixerGatedAttention(\n",
       "                (attn_layer): Linear(in_features=5, out_features=5, bias=True)\n",
       "                (attn_softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (feature_mixer): FeatureMixerBlock(\n",
       "              (norm): PatchTSMixerNormLayer(\n",
       "                (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (mlp): PatchTSMixerMLP(\n",
       "                (fc1): Linear(in_features=8, out_features=16, bias=True)\n",
       "                (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
       "                (dropout2): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gating_block): PatchTSMixerGatedAttention(\n",
       "                (attn_layer): Linear(in_features=8, out_features=8, bias=True)\n",
       "                (attn_softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (patching): PatchTSMixerPatchify()\n",
       "    (scaler): PatchTSMixerStdScaler()\n",
       "  )\n",
       "  (inject_scale): InjectScalerStatistics4D(\n",
       "    (inverse_trans_expansion): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (inverse_trans_compression): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (map_scale_expansion): Linear(in_features=2, out_features=4, bias=True)\n",
       "    (map_scale_compression): Linear(in_features=4, out_features=2, bias=True)\n",
       "  )\n",
       "  (head): PatchTSMixerLinearHead(\n",
       "    (projection): Linear(in_features=104, out_features=1, bias=True)\n",
       "    (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_TS.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS-PD regression\n",
    "## Time series & Point Distribution Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PatchTSMixerConfig, PatchTSMixerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TimesformerConfig, TimesformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_TS = PatchTSMixerConfig(context_length = 30, \n",
    "                            prediction_length = 1,\n",
    "                            num_input_channels = 13, \n",
    "                            d_model = 48,\n",
    "                            patch_len = 10,\n",
    "                            patch_stride = 5,\n",
    "                            use_positional_encoding = True,\n",
    "                            ca_d_model = 128\n",
    "                           )\n",
    "model_TS = PatchTSMixerModel(config_TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchTSMixerModel(\n",
       "  (encoder): PatchTSMixerEncoder(\n",
       "    (patcher): Linear(in_features=10, out_features=48, bias=True)\n",
       "    (positional_encoder): PatchTSMixerPositionalEncoding()\n",
       "    (mlp_mixer_encoder): PatchTSMixerBlock(\n",
       "      (mixers): ModuleList(\n",
       "        (0): PatchTSMixerLayer(\n",
       "          (patch_mixer): PatchMixerBlock(\n",
       "            (norm): PatchTSMixerNormLayer(\n",
       "              (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (mlp): PatchTSMixerMLP(\n",
       "              (fc1): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (dropout1): Dropout(p=0.2, inplace=False)\n",
       "              (fc2): Linear(in_features=10, out_features=5, bias=True)\n",
       "              (dropout2): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (gating_block): PatchTSMixerGatedAttention(\n",
       "              (attn_layer): Linear(in_features=5, out_features=5, bias=True)\n",
       "              (attn_softmax): Softmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "          (feature_mixer): FeatureMixerBlock(\n",
       "            (norm): PatchTSMixerNormLayer(\n",
       "              (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (mlp): PatchTSMixerMLP(\n",
       "              (fc1): Linear(in_features=48, out_features=96, bias=True)\n",
       "              (dropout1): Dropout(p=0.2, inplace=False)\n",
       "              (fc2): Linear(in_features=96, out_features=48, bias=True)\n",
       "              (dropout2): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (gating_block): PatchTSMixerGatedAttention(\n",
       "              (attn_layer): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (attn_softmax): Softmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PatchTSMixerLayer(\n",
       "          (patch_mixer): PatchMixerBlock(\n",
       "            (norm): PatchTSMixerNormLayer(\n",
       "              (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (mlp): PatchTSMixerMLP(\n",
       "              (fc1): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (dropout1): Dropout(p=0.2, inplace=False)\n",
       "              (fc2): Linear(in_features=10, out_features=5, bias=True)\n",
       "              (dropout2): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (gating_block): PatchTSMixerGatedAttention(\n",
       "              (attn_layer): Linear(in_features=5, out_features=5, bias=True)\n",
       "              (attn_softmax): Softmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "          (feature_mixer): FeatureMixerBlock(\n",
       "            (norm): PatchTSMixerNormLayer(\n",
       "              (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (mlp): PatchTSMixerMLP(\n",
       "              (fc1): Linear(in_features=48, out_features=96, bias=True)\n",
       "              (dropout1): Dropout(p=0.2, inplace=False)\n",
       "              (fc2): Linear(in_features=96, out_features=48, bias=True)\n",
       "              (dropout2): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (gating_block): PatchTSMixerGatedAttention(\n",
       "              (attn_layer): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (attn_softmax): Softmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): PatchTSMixerLayer(\n",
       "          (patch_mixer): PatchMixerBlock(\n",
       "            (norm): PatchTSMixerNormLayer(\n",
       "              (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (mlp): PatchTSMixerMLP(\n",
       "              (fc1): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (dropout1): Dropout(p=0.2, inplace=False)\n",
       "              (fc2): Linear(in_features=10, out_features=5, bias=True)\n",
       "              (dropout2): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (gating_block): PatchTSMixerGatedAttention(\n",
       "              (attn_layer): Linear(in_features=5, out_features=5, bias=True)\n",
       "              (attn_softmax): Softmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "          (feature_mixer): FeatureMixerBlock(\n",
       "            (norm): PatchTSMixerNormLayer(\n",
       "              (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (mlp): PatchTSMixerMLP(\n",
       "              (fc1): Linear(in_features=48, out_features=96, bias=True)\n",
       "              (dropout1): Dropout(p=0.2, inplace=False)\n",
       "              (fc2): Linear(in_features=96, out_features=48, bias=True)\n",
       "              (dropout2): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (gating_block): PatchTSMixerGatedAttention(\n",
       "              (attn_layer): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (attn_softmax): Softmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (patching): PatchTSMixerPatchify()\n",
       "  (scaler): PatchTSMixerStdScaler()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_TS.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 0.141MB\n"
     ]
    }
   ],
   "source": [
    "calculate_model_size(model_TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_TS\n",
    "x = torch.zeros((32, 30, 13)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model_TS.forward(past_values=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 13, 5, 48])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last_hidden_state - torch.FloatTensor\n",
    "# shape (batch_size, num_channels, num_patches, d_model)\n",
    "y.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PD: Point distribution\n",
    "config_PD = TimesformerConfig(image_size = 128,\n",
    "                             patch_size = 8,\n",
    "                             num_channels = 3,\n",
    "                             num_frames = 4,\n",
    "                             num_hidden_layers = 3,\n",
    "                             num_attention_heads = 12,\n",
    "                             hidden_size = 48,\n",
    "                             intermediate_size = 256,\n",
    "                             hidden_dropout_prob = 0.2)\n",
    "\n",
    "model_PD = TimesformerModel(config_PD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimesformerModel(\n",
       "  (embeddings): TimesformerEmbeddings(\n",
       "    (patch_embeddings): TimesformerPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 48, kernel_size=(8, 8), stride=(8, 8))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.2, inplace=False)\n",
       "    (time_drop): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (encoder): TimesformerEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=48, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=256, out_features=48, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=48, out_features=48, bias=True)\n",
       "      )\n",
       "      (1): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=48, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=256, out_features=48, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=48, out_features=48, bias=True)\n",
       "      )\n",
       "      (2): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=48, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=256, out_features=48, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=48, out_features=48, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_PD.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 0.614MB\n"
     ]
    }
   ],
   "source": [
    "calculate_model_size(model_PD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_PD \n",
    "# pixel_values - torch.FloatTensor of \n",
    "# shape (batch_size, num_frames, num_channels, height, width)\n",
    "x = torch.zeros((32, 4, 3, 96, 128)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model_PD.forward(pixel_values=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 769, 48])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.FloatTensor of shape \n",
    "# (batch_size, sequence_length, hidden_size))\n",
    "# (batch_size, 1 + H//P * W//P * T, hidden_size)\n",
    "yy = y.last_hidden_state\n",
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 48])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0][:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence length \n",
    "\n",
    "= 1 + #frames(4) x H/8 x W/8\n",
    "\n",
    "= 1 + 4 x 128/8 x 96/8\n",
    "\n",
    "= 1 + 64 x 12\n",
    "\n",
    "= 769\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 192, 4, 48])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy[:,1:, :].reshape([32, 192, 4, 48]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = torch.zeros((32, 30, 13)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = model.forward(past_values=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y.prediction_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-aware Cross-attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalCrossAttentionConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ## Time series specific configuration\n",
    "        ts_context_length: int = 30,\n",
    "        ts_patch_len: int = 5,\n",
    "        ts_num_input_channels: int = 1,\n",
    "        ts_patch_stride: int = 5,\n",
    "        ts_d_model: int = 32,\n",
    "        ts_time_step: int = 33*5,\n",
    "        ## Point distribution configuration\n",
    "        pd_num_frame: int = 4,\n",
    "        pd_patch_size: int = 8,\n",
    "        pd_height: int = 128,\n",
    "        pd_width: int = 96,\n",
    "        pd_d_model: int = 96,\n",
    "        pd_time_step: int = 33*10,\n",
    "        ## Time-preseved positional encoding\n",
    "        #pe_parameter = \n",
    "        ## General cross attention configuration\n",
    "        ca_d_model: int = 128,\n",
    "        ca_num_head: int = 8,\n",
    "        ca_dropout: float = 0.2,\n",
    "        ca_num_layers: int = 3,\n",
    "        # Classification/Regression configuration\n",
    "        reg_d_fc: int = 128,\n",
    "        reg_dropout: float = 0.2,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.ts_context_length = ts_context_length\n",
    "        self.ts_patch_len = ts_patch_len\n",
    "        self.ts_num_input_channels = ts_num_input_channels\n",
    "        self.ts_patch_stride = ts_patch_stride\n",
    "        self.ts_d_model = ts_d_model\n",
    "        self.ts_time_step = ts_time_step\n",
    "        ## Point distribution configuration\n",
    "        self.pd_num_frame = pd_num_frame\n",
    "        self.pd_patch_size = pd_patch_size\n",
    "        self.pd_height = pd_height\n",
    "        self.pd_width = pd_width\n",
    "        self.pd_d_model = pd_d_model\n",
    "        self.pd_time_step = pd_time_step\n",
    "        ## Time-preseved positional encoding\n",
    "        #pe_parameter = \n",
    "        ## General cross attention configuration\n",
    "        self.ca_d_model = ca_d_model\n",
    "        self.ca_num_head = ca_num_head\n",
    "        self.ca_dropout = ca_dropout\n",
    "        self.ca_num_layers = ca_num_layers\n",
    "        # Classification/Regression configuration\n",
    "        self.reg_d_fc = reg_d_fc\n",
    "        self.reg_dropout = reg_dropout\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesProjection(nn.Module):\n",
    "    def __init__(self, config: MultiModalCrossAttentionConfig):\n",
    "        super().__init__()\n",
    "        self.num_channels = config.ts_num_input_channels\n",
    "        self.ts_d_model = config.ts_d_model\n",
    "        self.ca_d_model = config.ca_d_model\n",
    "        \n",
    "        # dimension of all channel's hidden state in a patch\n",
    "        self.d_patch = self.num_channels * self.ts_d_model\n",
    "        self.projection = nn.Linear(self.d_patch, self.ca_d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, ts_hidden_state):\n",
    "        # ts_hidden_state.shape\n",
    "        # (batch_size, num_channels, num_patches, d_model)\n",
    "        # num_patches is in time-axis\n",
    "        batch_size = ts_hidden_state.shape[0]\n",
    "        num_patches = ts_hidden_state.shape[2]\n",
    "        ts_hidden_state = ts_hidden_state.permute(0,2,1,3).reshape(batch_size, \n",
    "                                                                   num_patches,\n",
    "                                                                   self.d_patch)\n",
    "        ts_hidden_state = self.projection(ts_hidden_state)\n",
    "        return ts_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointDistProjection(nn.Module):\n",
    "    def __init__(self, config: MultiModalCrossAttentionConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pd_num_frame = config.pd_num_frame\n",
    "        self.patch_width = config.pd_width // config.pd_patch_size\n",
    "        self.patch_height = config.pd_height // config.pd_patch_size\n",
    "        self.pd_d_model = config.pd_d_model\n",
    "        self.ca_d_model = config.ca_d_model\n",
    "        \n",
    "        # dimension of all patches' hidden state in a frame\n",
    "        d_frame = self.patch_height*self.patch_width*self.pd_d_model\n",
    "        self.projection = nn.Linear(d_frame, self.ca_d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, pd_hidden_state):\n",
    "        batch_size = pd_hidden_state.shape[0]\n",
    "        patch_height, patch_width = self.patch_height, self.patch_width\n",
    "        pd_num_frame = self.pd_num_frame\n",
    "        pd_d_model = self.pd_d_model\n",
    "        # pd_hidden_state.shape\n",
    "        # (batch_size, 1 + H//P * W//P * T, hidden_size)\n",
    "        # Drop the [CLS] token in the front\n",
    "        hidden_state = pd_hidden_state[:,1:,:]\n",
    "        # Time-preserved reshape\n",
    "        #print(\"PD Projection Shape {}\".format(hidden_state.shape))\n",
    "        hidden_state = hidden_state.view(batch_size, \n",
    "                                         patch_height, patch_width,\n",
    "                                         pd_num_frame, \n",
    "                                         pd_d_model\n",
    "                                        ).reshape(\n",
    "                                            batch_size,\n",
    "                                            patch_height*patch_width,\n",
    "                                            pd_num_frame,\n",
    "                                            pd_d_model\n",
    "                                         )\n",
    "        hidden_state = hidden_state.permute(0, 2, 1, 3)\n",
    "        hidden_state = hidden_state.reshape(batch_size, pd_num_frame, \n",
    "                                             patch_height*patch_width*pd_d_model)\n",
    "        # Project to (batch_size, num_frame, ca_d_model)\n",
    "        hidden_state = self.projection(hidden_state)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, config: MultiModalCrossAttentionConfig):\n",
    "        super().__init__()\n",
    "        self.pe_max_len = config.pe_max_len\n",
    "        self.ca_d_model = config.ca_d_model\n",
    "        \n",
    "        max_len = self.pe_max_len\n",
    "        d_model = self.ca_d_model\n",
    "        self.encoding = torch.zeros(max_len, d_model)#, device=\"cuda\")\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.cos(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.sin(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "        \n",
    "    def forward(self, x, time_step=33):\n",
    "        # time_step should be in millisecond\n",
    "        # For 30 FPS, it will be 33 millisecond \n",
    "        sample_pos = torch.arange(x.size(1), device=x.device).float()*time_step\n",
    "#         print(\"sample_pos device: {}\".format(sample_pos.get_device()))\n",
    "#         print(\"x device: {}\".format(x.get_device()))\n",
    "#         print(\"encoding device: {}\".format(self.encoding.get_device()))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoding = self.encoding[:, sample_pos.long().to(x.device), :].expand(x.size(0), -1, -1)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MultiModalCrossAttentionConfig(ts_context_length = 30,\n",
    "                                        ts_patch_len = 5,\n",
    "                                        ts_num_input_channels = 13,\n",
    "                                        ts_patch_stride = 5,\n",
    "                                        ts_d_model = 48,\n",
    "                                        ts_time_step = 33*5,\n",
    "                                        # PD parameter\n",
    "                                        pd_d_model = 48,\n",
    "                                        pd_time_step = 33*10,\n",
    "                                        pe_max_len = 10000, \n",
    "                                        # CA\n",
    "                                        ca_d_model = 48, \n",
    "                                        ca_num_head = 8,\n",
    "                                        ca_num_layers = 3\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, config: MultiModalCrossAttentionConfig):\n",
    "        super().__init__()\n",
    "        self.ca_d_model = config.ca_d_model\n",
    "        d_model = self.ca_d_model\n",
    "        self.ca_num_head = config.ca_num_head\n",
    "        num_head = self.ca_num_head\n",
    "        self.ca_dropout = config.ca_dropout\n",
    "        dropout = self.ca_dropout\n",
    "        # Attention\n",
    "        self.pd_attention = nn.MultiheadAttention(d_model, num_head, dropout=dropout, batch_first=True)\n",
    "        self.ts_attention = nn.MultiheadAttention(d_model, num_head, dropout=dropout, batch_first=True)\n",
    "        # Separate linear layers\n",
    "        self.pd_linear = nn.Linear(d_model, d_model)\n",
    "        self.ts_linear = nn.Linear(d_model, d_model)\n",
    "        # Normalization layers\n",
    "        self.pd_norm = nn.LayerNorm(d_model)\n",
    "        self.ts_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.pd_dropout = nn.Dropout(dropout)\n",
    "        self.ts_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, pd_hs, ts_hs):\n",
    "        # point distribution to time series cross attention\n",
    "        pd_to_ts, _ = self.pd_attention(pd_hs, ts_hs, ts_hs)\n",
    "        # time series to point distribution cross attention\n",
    "        ts_to_pd, _ = self.ts_attention(ts_hs, pd_hs, pd_hs)\n",
    "\n",
    "        # linearly transform the outputs\n",
    "        pd_hs = self.pd_linear(pd_to_ts)\n",
    "        pd_hs = self.pd_dropout(pd_hs)\n",
    "        pd_hs = self.pd_norm(pd_hs)\n",
    "        \n",
    "        ts_hs = self.ts_linear(ts_to_pd)\n",
    "        ts_hs = self.ts_dropout(ts_hs)\n",
    "        ts_hs = self.ts_norm(ts_hs)\n",
    "\n",
    "        return pd_hs, ts_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalCrossAttentionRegressor(nn.Module):\n",
    "    def __init__(self, config: MultiModalCrossAttentionConfig):\n",
    "        super().__init__()        \n",
    "        num_layers = config.ca_num_layers\n",
    "        ca_d_model = config.ca_d_model\n",
    "        reg_d_fc = config.reg_d_fc\n",
    "        reg_dropout = config.reg_dropout\n",
    "        \n",
    "        # Time-preserved projection layers\n",
    "        self.pd_proj = PointDistProjection(config)\n",
    "        self.ts_proj = TimeSeriesProjection(config)\n",
    "        # Postional encoding\n",
    "        self.ts_time_step = config.ts_time_step\n",
    "        self.pd_time_step = config.pd_time_step\n",
    "        self.pos_encoding = MultiModalPositionalEncoding(config)\n",
    "        # Attention|\n",
    "        self.ca_layers = nn.ModuleList([CrossAttentionLayer(config) for _ in range(num_layers)])\n",
    "        \n",
    "        self.fc1 = nn.Linear(ca_d_model*2, reg_d_fc)\n",
    "        self.fc_dropout = nn.Dropout(reg_dropout)\n",
    "        self.fc2 = nn.Linear(reg_d_fc, 1)\n",
    "    \n",
    "        \n",
    "    def forward(self, pd_hidden_state, ts_hidden_state):\n",
    "        # Project to the same dimension with time-preserve\n",
    "        pd_hs = self.pd_proj(pd_hidden_state)\n",
    "        ts_hs = self.ts_proj(ts_hidden_state)\n",
    "        \n",
    "        # Generate positional encodings\n",
    "        pd_pe = self.pos_encoding(pd_hs, self.pd_time_step)\n",
    "        ts_pe = self.pos_encoding(ts_hs, self.ts_time_step)\n",
    "        \n",
    "        # Add positional encodings\n",
    "        pd_hs = pd_hs + pd_pe\n",
    "        ts_hs = ts_hs + ts_pe\n",
    "        \n",
    "        for layer in self.ca_layers:\n",
    "            pd_hs, ts_hs = layer(pd_hs, ts_hs)\n",
    "            \n",
    "        # Concatenating the mean of each stream\n",
    "        # The mean pool eliminates/reduces the temporal axis(dim=1)\n",
    "        pd_hs_mean = pd_hs.mean(dim=1)\n",
    "        ts_hs_mean = ts_hs.mean(dim=1)\n",
    "        combined = torch.cat([pd_hs_mean, ts_hs_mean], dim=-1)\n",
    "        # combined.shape = [bs, d_modelx2]\n",
    "        out = self.fc1(combined)\n",
    "        out = nn.functional.gelu(out)\n",
    "        out = self.fc_dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_hs = torch.zeros([32, 13, 5, 48]).to(\"cuda\")\n",
    "pd_hs = torch.zeros([32, 769, 48]).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMCARegressor =  MultiModalCrossAttentionRegressor(config).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mMMCARegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_hs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_hs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/transformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[159], line 30\u001b[0m, in \u001b[0;36mMultiModalCrossAttentionRegressor.forward\u001b[0;34m(self, pd_hidden_state, ts_hidden_state)\u001b[0m\n\u001b[1;32m     27\u001b[0m ts_hs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mts_proj(ts_hidden_state)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Generate positional encodings\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m pd_pe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_hs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpd_time_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m ts_pe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(ts_hs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mts_time_step)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Add positional encodings\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/transformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[136], line 25\u001b[0m, in \u001b[0;36mMultiModalPositionalEncoding.forward\u001b[0;34m(self, x, time_step)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#         print(\"sample_pos device: {}\".format(sample_pos.get_device()))\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#         print(\"x device: {}\".format(x.get_device()))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#         print(\"encoding device: {}\".format(self.encoding.get_device()))\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m             encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_pos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encoding\n",
      "\u001b[0;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "y = MMCARegressor(pd_hs, ts_hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSEE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PatchTSMixerConfig, PatchTSMixerModel\n",
    "from transformers import TimesformerConfig, TimesformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PD: Point distribution\n",
    "config_PD = TimesformerConfig(image_size = 128,\n",
    "                             patch_size = 8,\n",
    "                             num_channels = 3,\n",
    "                             num_frames = 4,\n",
    "                             num_hidden_layers = 3,\n",
    "                             num_attention_heads = 12,\n",
    "                             hidden_size = 48,\n",
    "                             intermediate_size = 256,\n",
    "                             hidden_dropout_prob = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_TS = PatchTSMixerConfig(context_length = 30, \n",
    "                            prediction_length = 1,\n",
    "                            num_input_channels = 13, \n",
    "                            d_model = 48,\n",
    "                            patch_len = 10,\n",
    "                            patch_stride = 5,\n",
    "                            use_positional_encoding = True,\n",
    "                            ca_d_model = 128\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_CA = MultiModalCrossAttentionConfig(ts_context_length = 30,\n",
    "                                          ts_patch_len = 5,\n",
    "                                          ts_num_input_channels = 13,\n",
    "                                          ts_patch_stride = 5,\n",
    "                                          ts_d_model = 48,\n",
    "                                          ts_time_step = 33*5,\n",
    "                                          # PD parameter\n",
    "                                          pd_d_model = 48,\n",
    "                                          pd_time_step = 33*10,\n",
    "                                          pe_max_len = 10000, \n",
    "                                          # CA\n",
    "                                          ca_d_model = 48, \n",
    "                                          ca_num_head = 8,\n",
    "                                          ca_num_layers = 3\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSEEModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pd_config: TimesformerConfig,\n",
    "                 ts_config: PatchTSMixerConfig,\n",
    "                 ca_config: MultiModalCrossAttentionConfig):\n",
    "        super().__init__()        \n",
    "        \n",
    "        self.pd_config = pd_config\n",
    "        self.ts_config = ts_config\n",
    "        self.ca_config = ca_config\n",
    "        \n",
    "        self.pd_encoder = TimesformerModel(pd_config)\n",
    "        self.ts_encoder = PatchTSMixerModel(ts_config)\n",
    "        \n",
    "        self.ca_regressor = MultiModalCrossAttentionRegressor(ca_config)\n",
    "    \n",
    "        \n",
    "    def forward(self, point_dist, time_series):\n",
    "        \n",
    "        pd_hs = self.pd_encoder(point_dist).last_hidden_state\n",
    "        ts_hs = self.ts_encoder(time_series).last_hidden_state\n",
    "        \n",
    "        return self.ca_regressor(pd_hs, ts_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_TS\n",
    "x_ts = torch.zeros((1, 30, 13))#.to('cuda')\n",
    "x_pd = torch.zeros((1, 4, 3, 96, 128))#.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepSEEModel = DeepSEEModel(config_PD, config_TS, config_CA)#.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = deepSEEModel(x_pd, x_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 2.876MB\n"
     ]
    }
   ],
   "source": [
    "calculate_model_size(deepSEEModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyi/anaconda3/envs/transformer/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(deepSEEModel,\n",
    "                 args=(x_pd, x_ts),\n",
    "                 f=\"DeepSEE.onnx\",\n",
    "                 input_names=[\"pointDistInput\", \"timeSeriesInput\"],\n",
    "                 output_names=[\"poseError\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformer] *",
   "language": "python",
   "name": "conda-env-transformer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
